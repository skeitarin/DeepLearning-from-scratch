{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mini batch learning ---\n",
      " proc start : 2018/02/06 09:50:18\n",
      "- learning... 0/10000 - loss:6.89998873833\n",
      "- learning... 1/10000 - loss:6.88605895993\n",
      "- learning... 2/10000 - loss:6.90539565868\n",
      "- learning... 3/10000 - loss:6.90693539536\n",
      "- learning... 4/10000 - loss:6.90171537116\n",
      "- learning... 5/10000 - loss:6.90119571476\n",
      "- learning... 6/10000 - loss:6.88138613064\n",
      "- learning... 7/10000 - loss:6.88357942645\n",
      "- learning... 8/10000 - loss:6.89509459827\n",
      "- learning... 9/10000 - loss:6.90521467932\n",
      "- learning... 10/10000 - loss:6.91131374901\n",
      "- learning... 11/10000 - loss:6.90350582557\n",
      "- learning... 12/10000 - loss:6.90282196725\n",
      "- learning... 13/10000 - loss:6.90009376044\n",
      "- learning... 14/10000 - loss:6.89539086252\n",
      "- learning... 15/10000 - loss:6.90491143942\n",
      "- learning... 16/10000 - loss:6.89886380061\n",
      "- learning... 17/10000 - loss:6.90713412861\n",
      "- learning... 18/10000 - loss:6.90068627019\n",
      "- learning... 19/10000 - loss:6.89021039232\n",
      "- learning... 20/10000 - loss:6.90407090459\n",
      "- learning... 21/10000 - loss:6.88844789624\n",
      "- learning... 22/10000 - loss:6.90365408321\n",
      "- learning... 23/10000 - loss:6.90027815903\n",
      "- learning... 24/10000 - loss:6.90119694865\n",
      "- learning... 25/10000 - loss:6.8966931689\n",
      "- learning... 26/10000 - loss:6.90263784692\n",
      "- learning... 27/10000 - loss:6.90318944706\n",
      "- learning... 28/10000 - loss:6.88250825598\n",
      "- learning... 29/10000 - loss:6.9058538287\n",
      "- learning... 30/10000 - loss:6.89777226245\n",
      "- learning... 31/10000 - loss:6.89191200324\n",
      "- learning... 32/10000 - loss:6.90001771506\n",
      "- learning... 33/10000 - loss:6.89680690577\n",
      "- learning... 34/10000 - loss:6.89971045727\n",
      "- learning... 35/10000 - loss:6.89929919785\n",
      "- learning... 36/10000 - loss:6.88807511868\n",
      "- learning... 37/10000 - loss:6.90192539497\n",
      "- learning... 38/10000 - loss:6.89539334248\n",
      "- learning... 39/10000 - loss:6.87886571662\n",
      "- learning... 40/10000 - loss:6.8871790041\n",
      "- learning... 41/10000 - loss:6.90643830489\n",
      "- learning... 42/10000 - loss:6.88566840136\n",
      "- learning... 43/10000 - loss:6.90575071572\n",
      "- learning... 44/10000 - loss:6.89204127139\n",
      "- learning... 45/10000 - loss:6.90064446826\n",
      "- learning... 46/10000 - loss:6.89715854701\n",
      "- learning... 47/10000 - loss:6.88840829174\n",
      "- learning... 48/10000 - loss:6.89245142137\n",
      "- learning... 49/10000 - loss:6.88364556623\n",
      "- learning... 50/10000 - loss:6.90080642895\n",
      "- learning... 51/10000 - loss:6.89703807105\n",
      "- learning... 52/10000 - loss:6.89937329104\n",
      "- learning... 53/10000 - loss:6.89379282146\n",
      "- learning... 54/10000 - loss:6.89666840758\n",
      "- learning... 55/10000 - loss:6.87077432424\n",
      "- learning... 56/10000 - loss:6.89551387596\n",
      "- learning... 57/10000 - loss:6.88076284627\n",
      "- learning... 58/10000 - loss:6.88989903425\n",
      "- learning... 59/10000 - loss:6.90283842295\n",
      "- learning... 60/10000 - loss:6.86980390649\n",
      "- learning... 61/10000 - loss:6.90999930644\n",
      "- learning... 62/10000 - loss:6.89458116441\n",
      "- learning... 63/10000 - loss:6.8925288446\n",
      "- learning... 64/10000 - loss:6.88772727692\n",
      "- learning... 65/10000 - loss:6.88933028378\n",
      "- learning... 66/10000 - loss:6.88951208487\n",
      "- learning... 67/10000 - loss:6.89989891092\n",
      "- learning... 68/10000 - loss:6.89822326241\n",
      "- learning... 69/10000 - loss:6.89758629025\n",
      "- learning... 70/10000 - loss:6.89115905992\n",
      "- learning... 71/10000 - loss:6.89188664339\n",
      "- learning... 72/10000 - loss:6.89672054224\n",
      "- learning... 73/10000 - loss:6.88983872349\n",
      "- learning... 74/10000 - loss:6.89762159326\n",
      "- learning... 75/10000 - loss:6.88546311443\n",
      "- learning... 76/10000 - loss:6.8684153646\n",
      "- learning... 77/10000 - loss:6.88531539236\n",
      "- learning... 78/10000 - loss:6.88056818169\n",
      "- learning... 79/10000 - loss:6.90705739661\n",
      "- learning... 80/10000 - loss:6.87135008808\n",
      "- learning... 81/10000 - loss:6.88577203277\n",
      "- learning... 82/10000 - loss:6.89855760423\n",
      "- learning... 83/10000 - loss:6.8750621015\n",
      "- learning... 84/10000 - loss:6.90053840868\n",
      "- learning... 85/10000 - loss:6.90290642253\n",
      "- learning... 86/10000 - loss:6.88911139323\n",
      "- learning... 87/10000 - loss:6.88503368577\n",
      "- learning... 88/10000 - loss:6.89090781518\n",
      "- learning... 89/10000 - loss:6.8824419637\n",
      "- learning... 90/10000 - loss:6.89272975503\n",
      "- learning... 91/10000 - loss:6.89319939233\n",
      "- learning... 92/10000 - loss:6.88576391573\n",
      "- learning... 93/10000 - loss:6.88550447157\n",
      "- learning... 94/10000 - loss:6.88566636624\n",
      "- learning... 95/10000 - loss:6.8890919467\n",
      "- learning... 96/10000 - loss:6.89491369966\n",
      "- learning... 97/10000 - loss:6.87106861691\n",
      "- learning... 98/10000 - loss:6.88349246829\n",
      "- learning... 99/10000 - loss:6.89501983657\n",
      "- learning... 100/10000 - loss:6.87600588524\n",
      "- learning... 101/10000 - loss:6.86123337386\n",
      "- learning... 102/10000 - loss:6.89825292564\n",
      "- learning... 103/10000 - loss:6.89304053076\n",
      "- learning... 104/10000 - loss:6.89205836033\n",
      "- learning... 105/10000 - loss:6.88603200605\n",
      "- learning... 106/10000 - loss:6.87827604025\n",
      "- learning... 107/10000 - loss:6.88626856255\n",
      "- learning... 108/10000 - loss:6.88318051035\n",
      "- learning... 109/10000 - loss:6.86703541792\n",
      "- learning... 110/10000 - loss:6.88582138489\n",
      "- learning... 111/10000 - loss:6.89277340092\n",
      "- learning... 112/10000 - loss:6.881194657\n",
      "- learning... 113/10000 - loss:6.87414600566\n",
      "- learning... 114/10000 - loss:6.8920213374\n",
      "- learning... 115/10000 - loss:6.87277265255\n",
      "- learning... 116/10000 - loss:6.87953596658\n",
      "- learning... 117/10000 - loss:6.87876647074\n",
      "- learning... 118/10000 - loss:6.88534951454\n",
      "- learning... 119/10000 - loss:6.87286321338\n",
      "- learning... 120/10000 - loss:6.87128518861\n",
      "- learning... 121/10000 - loss:6.86580276776\n",
      "- learning... 122/10000 - loss:6.88125000419\n",
      "- learning... 123/10000 - loss:6.89156339506\n",
      "- learning... 124/10000 - loss:6.86335342273\n",
      "- learning... 125/10000 - loss:6.87368978433\n",
      "- learning... 126/10000 - loss:6.866849876\n",
      "- learning... 127/10000 - loss:6.88905383985\n",
      "- learning... 128/10000 - loss:6.87584962757\n",
      "- learning... 129/10000 - loss:6.86944257785\n",
      "- learning... 130/10000 - loss:6.88289281805\n",
      "- learning... 131/10000 - loss:6.85916718511\n",
      "- learning... 132/10000 - loss:6.8655088663\n",
      "- learning... 133/10000 - loss:6.87483542863\n",
      "- learning... 134/10000 - loss:6.87601217333\n",
      "- learning... 135/10000 - loss:6.85898396749\n",
      "- learning... 136/10000 - loss:6.86963673906\n",
      "- learning... 137/10000 - loss:6.86719303716\n",
      "- learning... 138/10000 - loss:6.85142558008\n",
      "- learning... 139/10000 - loss:6.84660374278\n",
      "- learning... 140/10000 - loss:6.85769607383\n",
      "- learning... 141/10000 - loss:6.8743567647\n",
      "- learning... 142/10000 - loss:6.86308486032\n",
      "- learning... 143/10000 - loss:6.85909036027\n",
      "- learning... 144/10000 - loss:6.8633067126\n",
      "- learning... 145/10000 - loss:6.84923729241\n",
      "- learning... 146/10000 - loss:6.86546875641\n",
      "- learning... 147/10000 - loss:6.85889913596\n",
      "- learning... 148/10000 - loss:6.85381725489\n",
      "- learning... 149/10000 - loss:6.85879011753\n",
      "- learning... 150/10000 - loss:6.86048976779\n",
      "- learning... 151/10000 - loss:6.84978460624\n",
      "- learning... 152/10000 - loss:6.84620890463\n",
      "- learning... 153/10000 - loss:6.86778801233\n",
      "- learning... 154/10000 - loss:6.8480970932\n",
      "- learning... 155/10000 - loss:6.87013345396\n",
      "- learning... 156/10000 - loss:6.8452137464\n",
      "- learning... 157/10000 - loss:6.84388595931\n",
      "- learning... 158/10000 - loss:6.85965576992\n",
      "- learning... 159/10000 - loss:6.83353034611\n",
      "- learning... 160/10000 - loss:6.86288462593\n",
      "- learning... 161/10000 - loss:6.83639960358\n",
      "- learning... 162/10000 - loss:6.8481735519\n",
      "- learning... 163/10000 - loss:6.85539081819\n",
      "- learning... 164/10000 - loss:6.84523830392\n",
      "- learning... 165/10000 - loss:6.85656573324\n",
      "- learning... 166/10000 - loss:6.83202875477\n",
      "- learning... 167/10000 - loss:6.82969803563\n",
      "- learning... 168/10000 - loss:6.8324399676\n",
      "- learning... 169/10000 - loss:6.81798459311\n",
      "- learning... 170/10000 - loss:6.82375672821\n",
      "- learning... 171/10000 - loss:6.82365010954\n",
      "- learning... 172/10000 - loss:6.82235414761\n",
      "- learning... 173/10000 - loss:6.82478156112\n",
      "- learning... 174/10000 - loss:6.85374496801\n",
      "- learning... 175/10000 - loss:6.83559580916\n",
      "- learning... 176/10000 - loss:6.82033190991\n",
      "- learning... 177/10000 - loss:6.79407645622\n",
      "- learning... 178/10000 - loss:6.81340379877\n",
      "- learning... 179/10000 - loss:6.82688147378\n",
      "- learning... 180/10000 - loss:6.8286118856\n",
      "- learning... 181/10000 - loss:6.80501456638\n",
      "- learning... 182/10000 - loss:6.80921938057\n",
      "- learning... 183/10000 - loss:6.81248826748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- learning... 184/10000 - loss:6.8091038726\n",
      "- learning... 185/10000 - loss:6.79088818839\n",
      "- learning... 186/10000 - loss:6.78510608215\n",
      "- learning... 187/10000 - loss:6.76867188311\n",
      "- learning... 188/10000 - loss:6.81526610213\n",
      "- learning... 189/10000 - loss:6.80286394147\n",
      "- learning... 190/10000 - loss:6.81193812098\n",
      "- learning... 191/10000 - loss:6.81359013608\n",
      "- learning... 192/10000 - loss:6.78690783845\n",
      "- learning... 193/10000 - loss:6.78940897998\n",
      "- learning... 194/10000 - loss:6.77820059667\n",
      "- learning... 195/10000 - loss:6.76690895152\n",
      "- learning... 196/10000 - loss:6.77184336708\n",
      "- learning... 197/10000 - loss:6.77665548618\n",
      "- learning... 198/10000 - loss:6.80922956159\n",
      "- learning... 199/10000 - loss:6.792958883\n",
      "- learning... 200/10000 - loss:6.77681538357\n",
      "- learning... 201/10000 - loss:6.78566993251\n",
      "- learning... 202/10000 - loss:6.75954952279\n",
      "- learning... 203/10000 - loss:6.7819493141\n",
      "- learning... 204/10000 - loss:6.77125075158\n",
      "- learning... 205/10000 - loss:6.77778690219\n",
      "- learning... 206/10000 - loss:6.72285559744\n",
      "- learning... 207/10000 - loss:6.74996292893\n",
      "- learning... 208/10000 - loss:6.77566656138\n",
      "- learning... 209/10000 - loss:6.75973843575\n",
      "- learning... 210/10000 - loss:6.76863153513\n",
      "- learning... 211/10000 - loss:6.76278089543\n",
      "- learning... 212/10000 - loss:6.77764346458\n",
      "- learning... 213/10000 - loss:6.74286600393\n",
      "- learning... 214/10000 - loss:6.76218354553\n",
      "- learning... 215/10000 - loss:6.7296605174\n",
      "- learning... 216/10000 - loss:6.74273295458\n",
      "- learning... 217/10000 - loss:6.72702074515\n",
      "- learning... 218/10000 - loss:6.74815858034\n",
      "- learning... 219/10000 - loss:6.72202930101\n",
      "- learning... 220/10000 - loss:6.74744727605\n",
      "- learning... 221/10000 - loss:6.69468459317\n",
      "- learning... 222/10000 - loss:6.73212368031\n",
      "- learning... 223/10000 - loss:6.70996235139\n",
      "- learning... 224/10000 - loss:6.69834380374\n",
      "- learning... 225/10000 - loss:6.69244539587\n",
      "- learning... 226/10000 - loss:6.70828705661\n",
      "- learning... 227/10000 - loss:6.72908724466\n",
      "- learning... 228/10000 - loss:6.68699750646\n",
      "- learning... 229/10000 - loss:6.69052344652\n",
      "- learning... 230/10000 - loss:6.68197018401\n",
      "- learning... 231/10000 - loss:6.65790409392\n",
      "- learning... 232/10000 - loss:6.66041397997\n",
      "- learning... 233/10000 - loss:6.68872924195\n",
      "- learning... 234/10000 - loss:6.70130027154\n",
      "- learning... 235/10000 - loss:6.67646358829\n",
      "- learning... 236/10000 - loss:6.68206459522\n",
      "- learning... 237/10000 - loss:6.66192581777\n",
      "- learning... 238/10000 - loss:6.6512916635\n",
      "- learning... 239/10000 - loss:6.67934083265\n",
      "- learning... 240/10000 - loss:6.63488862103\n",
      "- learning... 241/10000 - loss:6.66642091665\n",
      "- learning... 242/10000 - loss:6.6380404748\n",
      "- learning... 243/10000 - loss:6.62852411396\n",
      "- learning... 244/10000 - loss:6.67741564597\n",
      "- learning... 245/10000 - loss:6.62696644162\n",
      "- learning... 246/10000 - loss:6.55590696102\n",
      "- learning... 247/10000 - loss:6.59272845627\n",
      "- learning... 248/10000 - loss:6.61597963115\n",
      "- learning... 249/10000 - loss:6.62581344047\n",
      "- learning... 250/10000 - loss:6.62216382128\n",
      "- learning... 251/10000 - loss:6.66194105196\n",
      "- learning... 252/10000 - loss:6.64327291364\n",
      "- learning... 253/10000 - loss:6.61877807787\n",
      "- learning... 254/10000 - loss:6.56986812404\n",
      "- learning... 255/10000 - loss:6.59221322307\n",
      "- learning... 256/10000 - loss:6.62515762875\n",
      "- learning... 257/10000 - loss:6.56039100494\n",
      "- learning... 258/10000 - loss:6.56809884763\n",
      "- learning... 259/10000 - loss:6.57336311509\n",
      "- learning... 260/10000 - loss:6.55199802133\n",
      "- learning... 261/10000 - loss:6.55122444187\n",
      "- learning... 262/10000 - loss:6.55690719398\n",
      "- learning... 263/10000 - loss:6.62059748885\n",
      "- learning... 264/10000 - loss:6.54127604667\n",
      "- learning... 265/10000 - loss:6.50397798067\n",
      "- learning... 266/10000 - loss:6.44172083157\n",
      "- learning... 267/10000 - loss:6.5262038945\n",
      "- learning... 268/10000 - loss:6.51429242069\n",
      "- learning... 269/10000 - loss:6.55333082321\n",
      "- learning... 270/10000 - loss:6.53419414944\n",
      "- learning... 271/10000 - loss:6.49684518821\n",
      "- learning... 272/10000 - loss:6.48734717422\n",
      "- learning... 273/10000 - loss:6.49691821805\n",
      "- learning... 274/10000 - loss:6.42808897764\n",
      "- learning... 275/10000 - loss:6.47923974373\n",
      "- learning... 276/10000 - loss:6.52955468368\n",
      "- learning... 277/10000 - loss:6.46319286067\n",
      "- learning... 278/10000 - loss:6.47964773602\n",
      "- learning... 279/10000 - loss:6.51355029547\n",
      "- learning... 280/10000 - loss:6.49711098459\n",
      "- learning... 281/10000 - loss:6.47086844802\n",
      "- learning... 282/10000 - loss:6.38388689577\n",
      "- learning... 283/10000 - loss:6.44754270493\n",
      "- learning... 284/10000 - loss:6.46775438162\n",
      "- learning... 285/10000 - loss:6.41878096786\n",
      "- learning... 286/10000 - loss:6.4561229149\n",
      "- learning... 287/10000 - loss:6.41199105135\n",
      "- learning... 288/10000 - loss:6.44407532517\n",
      "- learning... 289/10000 - loss:6.43501070278\n",
      "- learning... 290/10000 - loss:6.43217942618\n",
      "- learning... 291/10000 - loss:6.43078655849\n",
      "- learning... 292/10000 - loss:6.35204277778\n",
      "- learning... 293/10000 - loss:6.4367430879\n",
      "- learning... 294/10000 - loss:6.39543473127\n",
      "- learning... 295/10000 - loss:6.39131190443\n",
      "- learning... 296/10000 - loss:6.32817590206\n",
      "- learning... 297/10000 - loss:6.39262321469\n",
      "- learning... 298/10000 - loss:6.38470649097\n",
      "- learning... 299/10000 - loss:6.35764723884\n",
      "- learning... 300/10000 - loss:6.39520662128\n",
      "- learning... 301/10000 - loss:6.39527593035\n",
      "- learning... 302/10000 - loss:6.34893681636\n",
      "- learning... 303/10000 - loss:6.34879417429\n",
      "- learning... 304/10000 - loss:6.31859108319\n",
      "- learning... 305/10000 - loss:6.32696781753\n",
      "- learning... 306/10000 - loss:6.35724756822\n",
      "- learning... 307/10000 - loss:6.29746235006\n",
      "- learning... 308/10000 - loss:6.25808137576\n",
      "- learning... 309/10000 - loss:6.35013356982\n",
      "- learning... 310/10000 - loss:6.29730429507\n",
      "- learning... 311/10000 - loss:6.36075483431\n",
      "- learning... 312/10000 - loss:6.25157411921\n",
      "- learning... 313/10000 - loss:6.26181041692\n",
      "- learning... 314/10000 - loss:6.32012710179\n",
      "- learning... 315/10000 - loss:6.26933302293\n",
      "- learning... 316/10000 - loss:6.32041658912\n",
      "- learning... 317/10000 - loss:6.24901449054\n",
      "- learning... 318/10000 - loss:6.29253303224\n",
      "- learning... 319/10000 - loss:6.29116803905\n",
      "- learning... 320/10000 - loss:6.33823129423\n",
      "- learning... 321/10000 - loss:6.37974162711\n",
      "- learning... 322/10000 - loss:6.20645047881\n",
      "- learning... 323/10000 - loss:6.23980179161\n",
      "- learning... 324/10000 - loss:6.23102072256\n",
      "- learning... 325/10000 - loss:6.26637148378\n",
      "- learning... 326/10000 - loss:6.28816273674\n",
      "- learning... 327/10000 - loss:6.30463105111\n",
      "- learning... 328/10000 - loss:6.28883209179\n",
      "- learning... 329/10000 - loss:6.2519981861\n",
      "- learning... 330/10000 - loss:6.12560953288\n",
      "- learning... 331/10000 - loss:6.23475608926\n",
      "- learning... 332/10000 - loss:6.20082511302\n",
      "- learning... 333/10000 - loss:6.22751997929\n",
      "- learning... 334/10000 - loss:6.213383543\n",
      "- learning... 335/10000 - loss:6.19764011448\n",
      "- learning... 336/10000 - loss:6.14446694264\n",
      "- learning... 337/10000 - loss:6.17708963011\n",
      "- learning... 338/10000 - loss:6.11878245873\n",
      "- learning... 339/10000 - loss:6.18674671547\n",
      "- learning... 340/10000 - loss:6.18129864591\n",
      "- learning... 341/10000 - loss:6.24977530829\n",
      "- learning... 342/10000 - loss:6.2336196666\n",
      "- learning... 343/10000 - loss:6.17012693694\n",
      "- learning... 344/10000 - loss:6.18341756278\n",
      "- learning... 345/10000 - loss:6.12507664289\n",
      "- learning... 346/10000 - loss:6.12068398108\n",
      "- learning... 347/10000 - loss:6.21010848098\n",
      "- learning... 348/10000 - loss:6.047913019\n",
      "- learning... 349/10000 - loss:6.14914038155\n",
      "- learning... 350/10000 - loss:6.17988255562\n",
      "- learning... 351/10000 - loss:6.17325434874\n",
      "- learning... 352/10000 - loss:6.21411805358\n",
      "- learning... 353/10000 - loss:6.19895170049\n",
      "- learning... 354/10000 - loss:6.07845336554\n",
      "- learning... 355/10000 - loss:6.15722387229\n",
      "- learning... 356/10000 - loss:6.1818889424\n",
      "- learning... 357/10000 - loss:6.13613257723\n",
      "- learning... 358/10000 - loss:6.09643804311\n",
      "- learning... 359/10000 - loss:6.13032603479\n",
      "- learning... 360/10000 - loss:6.10526939488\n",
      "- learning... 361/10000 - loss:6.08296608574\n",
      "- learning... 362/10000 - loss:6.13115593737\n",
      "- learning... 363/10000 - loss:6.10252593706\n",
      "- learning... 364/10000 - loss:6.13012610306\n",
      "- learning... 365/10000 - loss:5.996955631\n",
      "- learning... 366/10000 - loss:6.04774276871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- learning... 367/10000 - loss:5.9846042815\n",
      "- learning... 368/10000 - loss:6.05279416148\n",
      "- learning... 369/10000 - loss:6.08410253501\n",
      "- learning... 370/10000 - loss:6.05685394931\n",
      "- learning... 371/10000 - loss:6.17318202766\n",
      "- learning... 372/10000 - loss:6.06188822145\n",
      "- learning... 373/10000 - loss:5.99270512171\n",
      "- learning... 374/10000 - loss:5.98235949296\n",
      "- learning... 375/10000 - loss:5.99508426439\n",
      "- learning... 376/10000 - loss:6.1338498802\n",
      "- learning... 377/10000 - loss:5.90593697483\n",
      "- learning... 378/10000 - loss:6.00529708501\n",
      "- learning... 379/10000 - loss:5.99841485708\n",
      "- learning... 380/10000 - loss:5.9887019663\n",
      "- learning... 381/10000 - loss:6.01239328928\n",
      "- learning... 382/10000 - loss:6.02129149881\n",
      "- learning... 383/10000 - loss:5.91874498729\n",
      "- learning... 384/10000 - loss:5.92631636667\n",
      "- learning... 385/10000 - loss:5.97681360025\n",
      "- learning... 386/10000 - loss:6.03735903747\n",
      "- learning... 387/10000 - loss:6.03888175264\n",
      "- learning... 388/10000 - loss:5.98755522266\n",
      "- learning... 389/10000 - loss:5.95347693617\n",
      "- learning... 390/10000 - loss:6.03019425808\n",
      "- learning... 391/10000 - loss:6.120908058\n",
      "- learning... 392/10000 - loss:6.0313330565\n",
      "- learning... 393/10000 - loss:5.95826524337\n",
      "- learning... 394/10000 - loss:6.01271697167\n",
      "- learning... 395/10000 - loss:5.95716384628\n",
      "- learning... 396/10000 - loss:5.83578710838\n",
      "- learning... 397/10000 - loss:6.05406055591\n",
      "- learning... 398/10000 - loss:5.9205383675\n",
      "- learning... 399/10000 - loss:5.97619596491\n",
      "- learning... 400/10000 - loss:5.90743604564\n",
      "- learning... 401/10000 - loss:5.90697218884\n",
      "- learning... 402/10000 - loss:5.93888130311\n",
      "- learning... 403/10000 - loss:5.91601782875\n",
      "- learning... 404/10000 - loss:5.91647920872\n",
      "- learning... 405/10000 - loss:6.00833526121\n",
      "- learning... 406/10000 - loss:5.91824920876\n",
      "- learning... 407/10000 - loss:5.95719740476\n",
      "- learning... 408/10000 - loss:6.0359122293\n",
      "- learning... 409/10000 - loss:5.91836057549\n",
      "- learning... 410/10000 - loss:5.91623239498\n",
      "- learning... 411/10000 - loss:6.02414178759\n",
      "- learning... 412/10000 - loss:5.96447633934\n",
      "- learning... 413/10000 - loss:5.84187978347\n",
      "- learning... 414/10000 - loss:5.92864533732\n",
      "- learning... 415/10000 - loss:5.92507926955\n",
      "- learning... 416/10000 - loss:5.93141790486\n",
      "- learning... 417/10000 - loss:5.89579366646\n",
      "- learning... 418/10000 - loss:5.8752576552\n",
      "- learning... 419/10000 - loss:5.93152082674\n",
      "- learning... 420/10000 - loss:5.97209327709\n",
      "- learning... 421/10000 - loss:5.95633797975\n",
      "- learning... 422/10000 - loss:5.81456832905\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"./deep-learning-from-scratch-master/\")\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from datetime import datetime\n",
    "\n",
    "class TwoLayerNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "    \n",
    "    def sigmoid_function(self, x):\n",
    "        return 1 / (1 + np.exp(-x)) # exp(x)はネイピア数（e）のx乗\n",
    "\n",
    "    def softmax_function(self, a):\n",
    "        c = np.max(a)\n",
    "        e_a = np.exp(a - c) # オーバーフロー対策\n",
    "        sum_e_a = np.sum(e_a)\n",
    "        return e_a / sum_e_a\n",
    "\n",
    "    def cross_entropy_error(self, y, t):\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1) \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params[\"W1\"], self.params[\"W2\"]\n",
    "        b1, b2 = self.params[\"b1\"], self.params[\"b2\"]\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = self.sigmoid_function(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = self.softmax_function(a2)\n",
    "        return y\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, f, x):\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h # self.params[\"x\"]の更新（numpyのため参照渡しになる）\n",
    "            fxh1 = f(x) # f(x+h) -> xはダミーの引数。loss引数xには、gradient引数xが渡される\n",
    "        \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x) # f(x-h)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "            x[idx] = tmp_val # 値を元に戻す\n",
    "            it.iternext()   \n",
    "        return grad\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = self.numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads[\"b1\"] = self.numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads[\"W2\"] = self.numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads[\"b2\"] = self.numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "        return grads\n",
    "\n",
    "# NN = TwoLayerNN(input_size=784, hidden_size=100, output_size=10)\n",
    "# x = np.random.rand(100, 784)\n",
    "# t = np.random.rand(100, 10)\n",
    "# y = NN.predict(x)\n",
    "# grads = NN.gradient(x, t)\n",
    "# print(grads[\"W1\"].shape)\n",
    "\n",
    "# --- ミニバッチ学習 ---\n",
    "print(\"--- mini batch learning ---\")\n",
    "print(\" proc start : \" + str(datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")))\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# ハイパーパラメータ\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 1エポックあたりの繰り返し数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "NN = TwoLayerNN(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # ミニバッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    # print(\"batch_mask : \" + str(batch_mask))\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 勾配の計算\n",
    "    grads = NN.gradient(x_batch, t_batch)\n",
    "\n",
    "    # パラメータの更新\n",
    "    for key in (\"W1\", \"W2\", \"b1\", \"b2\"):\n",
    "        NN.params[key] = NN.params[key] - (learning_rate * grads[key])\n",
    "    \n",
    "    # 学習経過の記録\n",
    "    loss = NN.loss(x_batch, t_batch)\n",
    "    print(\"- learning... \" + str(i) + \"/\" + str(iters_num) + \" - loss:\" + str(loss))\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1エポックごとに認識精度を計算\n",
    "    if 1 % iter_per_epoch == 0:\n",
    "        train_acc_list.append(NN.accuracy(x_train, t_train))\n",
    "        test_acc_list.append(NN.accuracy(x_train, t_train))\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "print(\" proc end : \" + str(datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\")))\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
