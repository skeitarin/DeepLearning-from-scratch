{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- mini batch learning ---\n",
      "- learning... 0/10000 - loss:6.896762212875149\n",
      "- learning... 1/10000 - loss:6.898006404399876\n",
      "- learning... 2/10000 - loss:6.894620930755418\n",
      "- learning... 3/10000 - loss:6.901588437853344\n",
      "- learning... 4/10000 - loss:6.89541504056809\n",
      "- learning... 5/10000 - loss:6.892621106055443\n",
      "- learning... 6/10000 - loss:6.903155101180779\n",
      "- learning... 7/10000 - loss:6.902643701905974\n",
      "- learning... 8/10000 - loss:6.889317707226538\n",
      "- learning... 9/10000 - loss:6.883500931752714\n",
      "- learning... 10/10000 - loss:6.88435897837484\n",
      "- learning... 11/10000 - loss:6.902383893462493\n",
      "- learning... 12/10000 - loss:6.897598498087332\n",
      "- learning... 13/10000 - loss:6.881720890875175\n",
      "- learning... 14/10000 - loss:6.910850983697419\n",
      "- learning... 15/10000 - loss:6.901288939374151\n",
      "- learning... 16/10000 - loss:6.901853582080626\n",
      "- learning... 17/10000 - loss:6.901134035340448\n",
      "- learning... 18/10000 - loss:6.883268235944054\n",
      "- learning... 19/10000 - loss:6.904590399869263\n",
      "- learning... 20/10000 - loss:6.90017716678603\n",
      "- learning... 21/10000 - loss:6.903272181803291\n",
      "- learning... 22/10000 - loss:6.896997758503984\n",
      "- learning... 23/10000 - loss:6.896673154530026\n",
      "- learning... 24/10000 - loss:6.8967354030383925\n",
      "- learning... 25/10000 - loss:6.899718998905861\n",
      "- learning... 26/10000 - loss:6.894232654899961\n",
      "- learning... 27/10000 - loss:6.883705416234214\n",
      "- learning... 28/10000 - loss:6.902414758389782\n",
      "- learning... 29/10000 - loss:6.900601042035822\n",
      "- learning... 30/10000 - loss:6.898767085603606\n",
      "- learning... 31/10000 - loss:6.890557874830551\n",
      "- learning... 32/10000 - loss:6.888572199329917\n",
      "- learning... 33/10000 - loss:6.898983372729427\n",
      "- learning... 34/10000 - loss:6.90741319247398\n",
      "- learning... 35/10000 - loss:6.884255074839106\n",
      "- learning... 36/10000 - loss:6.902984990493455\n",
      "- learning... 37/10000 - loss:6.898624372319639\n",
      "- learning... 38/10000 - loss:6.887975556889421\n",
      "- learning... 39/10000 - loss:6.901907063898399\n",
      "- learning... 40/10000 - loss:6.907157885466624\n",
      "- learning... 41/10000 - loss:6.890638582459523\n",
      "- learning... 42/10000 - loss:6.887401902584028\n",
      "- learning... 43/10000 - loss:6.876031915951598\n",
      "- learning... 44/10000 - loss:6.872086197368751\n",
      "- learning... 45/10000 - loss:6.916575071681272\n",
      "- learning... 46/10000 - loss:6.896017898638459\n",
      "- learning... 47/10000 - loss:6.891417129270731\n",
      "- learning... 48/10000 - loss:6.898533797917095\n",
      "- learning... 49/10000 - loss:6.896489324396168\n",
      "- learning... 50/10000 - loss:6.888140876403705\n",
      "- learning... 51/10000 - loss:6.882760307192207\n",
      "- learning... 52/10000 - loss:6.902707849662146\n",
      "- learning... 53/10000 - loss:6.901324284294705\n",
      "- learning... 54/10000 - loss:6.879563580223034\n",
      "- learning... 55/10000 - loss:6.892545998156453\n",
      "- learning... 56/10000 - loss:6.8938504176694115\n",
      "- learning... 57/10000 - loss:6.89666912023163\n",
      "- learning... 58/10000 - loss:6.89366168115615\n",
      "- learning... 59/10000 - loss:6.903078401981993\n",
      "- learning... 60/10000 - loss:6.890637448166185\n",
      "- learning... 61/10000 - loss:6.885241023325541\n",
      "- learning... 62/10000 - loss:6.8990073617723935\n",
      "- learning... 63/10000 - loss:6.903904627953438\n",
      "- learning... 64/10000 - loss:6.892870520561533\n",
      "- learning... 65/10000 - loss:6.875630305435757\n",
      "- learning... 66/10000 - loss:6.888893265559821\n",
      "- learning... 67/10000 - loss:6.881619596729848\n",
      "- learning... 68/10000 - loss:6.883729187902565\n",
      "- learning... 69/10000 - loss:6.89262878600028\n",
      "- learning... 70/10000 - loss:6.8984900944680065\n",
      "- learning... 71/10000 - loss:6.886506838559276\n",
      "- learning... 72/10000 - loss:6.894103225687411\n",
      "- learning... 73/10000 - loss:6.891650939730701\n",
      "- learning... 74/10000 - loss:6.894872724292323\n",
      "- learning... 75/10000 - loss:6.889881490663459\n",
      "- learning... 76/10000 - loss:6.875995097973671\n",
      "- learning... 77/10000 - loss:6.897832684529322\n",
      "- learning... 78/10000 - loss:6.881959322605286\n",
      "- learning... 79/10000 - loss:6.890548589842247\n",
      "- learning... 80/10000 - loss:6.888906058086353\n",
      "- learning... 81/10000 - loss:6.904645498600822\n",
      "- learning... 82/10000 - loss:6.901974438851532\n",
      "- learning... 83/10000 - loss:6.883003822865001\n",
      "- learning... 84/10000 - loss:6.874650338803804\n",
      "- learning... 85/10000 - loss:6.878788615928497\n",
      "- learning... 86/10000 - loss:6.889795414804536\n",
      "- learning... 87/10000 - loss:6.891945132607191\n",
      "- learning... 88/10000 - loss:6.8914709004751185\n",
      "- learning... 89/10000 - loss:6.875218388855538\n",
      "- learning... 90/10000 - loss:6.878274895170858\n",
      "- learning... 91/10000 - loss:6.88389131655868\n",
      "- learning... 92/10000 - loss:6.882333647360327\n",
      "- learning... 93/10000 - loss:6.893744079385672\n",
      "- learning... 94/10000 - loss:6.87076454906557\n",
      "- learning... 95/10000 - loss:6.887550904155417\n",
      "- learning... 96/10000 - loss:6.865706147674343\n",
      "- learning... 97/10000 - loss:6.895147681787507\n",
      "- learning... 98/10000 - loss:6.886151128561261\n",
      "- learning... 99/10000 - loss:6.8884314373385305\n",
      "- learning... 100/10000 - loss:6.8877311344522445\n",
      "- learning... 101/10000 - loss:6.876894504177048\n",
      "- learning... 102/10000 - loss:6.862928573424522\n",
      "- learning... 103/10000 - loss:6.878149048467662\n",
      "- learning... 104/10000 - loss:6.888089592951729\n",
      "- learning... 105/10000 - loss:6.876601102724132\n",
      "- learning... 106/10000 - loss:6.899335583836506\n",
      "- learning... 107/10000 - loss:6.880990387609087\n",
      "- learning... 108/10000 - loss:6.865519235019994\n",
      "- learning... 109/10000 - loss:6.883162871550619\n",
      "- learning... 110/10000 - loss:6.888791971898772\n",
      "- learning... 111/10000 - loss:6.884903642236611\n",
      "- learning... 112/10000 - loss:6.884471292441922\n",
      "- learning... 113/10000 - loss:6.884866529198754\n",
      "- learning... 114/10000 - loss:6.878008730057393\n",
      "- learning... 115/10000 - loss:6.879705724419609\n",
      "- learning... 116/10000 - loss:6.886010994528059\n",
      "- learning... 117/10000 - loss:6.84905482438262\n",
      "- learning... 118/10000 - loss:6.864062777356018\n",
      "- learning... 119/10000 - loss:6.866565601315305\n",
      "- learning... 120/10000 - loss:6.8794331723631625\n",
      "- learning... 121/10000 - loss:6.859262320361104\n",
      "- learning... 122/10000 - loss:6.879702840933923\n",
      "- learning... 123/10000 - loss:6.897678971812984\n",
      "- learning... 124/10000 - loss:6.871074905785681\n",
      "- learning... 125/10000 - loss:6.8769796007379105\n",
      "- learning... 126/10000 - loss:6.874439350354727\n",
      "- learning... 127/10000 - loss:6.843898852971466\n",
      "- learning... 128/10000 - loss:6.844259351179935\n",
      "- learning... 129/10000 - loss:6.8787563812999055\n",
      "- learning... 130/10000 - loss:6.861358831780645\n",
      "- learning... 131/10000 - loss:6.8616992936756445\n",
      "- learning... 132/10000 - loss:6.8670308200178\n",
      "- learning... 133/10000 - loss:6.854953169991081\n",
      "- learning... 134/10000 - loss:6.87735790446147\n",
      "- learning... 135/10000 - loss:6.877001612269296\n",
      "- learning... 136/10000 - loss:6.868065538786826\n",
      "- learning... 137/10000 - loss:6.865694779497611\n",
      "- learning... 138/10000 - loss:6.864703141109865\n",
      "- learning... 139/10000 - loss:6.855144038355413\n",
      "- learning... 140/10000 - loss:6.880057185748408\n",
      "- learning... 141/10000 - loss:6.8619917279358775\n",
      "- learning... 142/10000 - loss:6.866448103019693\n",
      "- learning... 143/10000 - loss:6.863097555203959\n",
      "- learning... 144/10000 - loss:6.837248118000786\n",
      "- learning... 145/10000 - loss:6.87925375149498\n",
      "- learning... 146/10000 - loss:6.8634262200792895\n",
      "- learning... 147/10000 - loss:6.849505421522266\n",
      "- learning... 148/10000 - loss:6.870768669784775\n",
      "- learning... 149/10000 - loss:6.857832101094974\n",
      "- learning... 150/10000 - loss:6.850189665420578\n",
      "- learning... 151/10000 - loss:6.853153681060263\n",
      "- learning... 152/10000 - loss:6.854528223877203\n",
      "- learning... 153/10000 - loss:6.853600654971431\n",
      "- learning... 154/10000 - loss:6.842660189073642\n",
      "- learning... 155/10000 - loss:6.8395540936581085\n",
      "- learning... 156/10000 - loss:6.827061211497726\n",
      "- learning... 157/10000 - loss:6.8631994239341045\n",
      "- learning... 158/10000 - loss:6.830843607535935\n",
      "- learning... 159/10000 - loss:6.859880224233661\n",
      "- learning... 160/10000 - loss:6.83607596275694\n",
      "- learning... 161/10000 - loss:6.847238867274699\n",
      "- learning... 162/10000 - loss:6.850284505550244\n",
      "- learning... 163/10000 - loss:6.855921470163645\n",
      "- learning... 164/10000 - loss:6.85097928116503\n",
      "- learning... 165/10000 - loss:6.834429016498267\n",
      "- learning... 166/10000 - loss:6.841305872443774\n",
      "- learning... 167/10000 - loss:6.846936688789319\n",
      "- learning... 168/10000 - loss:6.8294542487222625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- learning... 169/10000 - loss:6.823468749668662\n",
      "- learning... 170/10000 - loss:6.817013387501099\n",
      "- learning... 171/10000 - loss:6.82776033258393\n",
      "- learning... 172/10000 - loss:6.840308999097334\n",
      "- learning... 173/10000 - loss:6.805097979379337\n",
      "- learning... 174/10000 - loss:6.83363313174849\n",
      "- learning... 175/10000 - loss:6.840761438711444\n",
      "- learning... 176/10000 - loss:6.8185875782094225\n",
      "- learning... 177/10000 - loss:6.832206965211265\n",
      "- learning... 178/10000 - loss:6.801072776867934\n",
      "- learning... 179/10000 - loss:6.828847550453953\n",
      "- learning... 180/10000 - loss:6.817348691710041\n",
      "- learning... 181/10000 - loss:6.774421315069125\n",
      "- learning... 182/10000 - loss:6.806578392974392\n",
      "- learning... 183/10000 - loss:6.799740759849719\n",
      "- learning... 184/10000 - loss:6.820424224493957\n",
      "- learning... 185/10000 - loss:6.8157973819414535\n",
      "- learning... 186/10000 - loss:6.783481507522687\n",
      "- learning... 187/10000 - loss:6.790345917532278\n",
      "- learning... 188/10000 - loss:6.782087831786989\n",
      "- learning... 189/10000 - loss:6.795728640000067\n",
      "- learning... 190/10000 - loss:6.79112108666401\n",
      "- learning... 191/10000 - loss:6.7714702930530235\n",
      "- learning... 192/10000 - loss:6.780805125571857\n",
      "- learning... 193/10000 - loss:6.801928400102455\n",
      "- learning... 194/10000 - loss:6.796105626350645\n",
      "- learning... 195/10000 - loss:6.790545612916492\n",
      "- learning... 196/10000 - loss:6.811707181637425\n",
      "- learning... 197/10000 - loss:6.750642043755366\n",
      "- learning... 198/10000 - loss:6.773947343790984\n",
      "- learning... 199/10000 - loss:6.739879876693259\n",
      "- learning... 200/10000 - loss:6.777649705604441\n",
      "- learning... 201/10000 - loss:6.775035859981922\n",
      "- learning... 202/10000 - loss:6.7462911471971845\n",
      "- learning... 203/10000 - loss:6.7638129327246235\n",
      "- learning... 204/10000 - loss:6.762405666483645\n",
      "- learning... 205/10000 - loss:6.759057046927265\n",
      "- learning... 206/10000 - loss:6.747254307163165\n",
      "- learning... 207/10000 - loss:6.751732035243675\n",
      "- learning... 208/10000 - loss:6.755939038227989\n",
      "- learning... 209/10000 - loss:6.713775865840515\n",
      "- learning... 210/10000 - loss:6.792506927730673\n",
      "- learning... 211/10000 - loss:6.761323763739649\n",
      "- learning... 212/10000 - loss:6.733488421935082\n",
      "- learning... 213/10000 - loss:6.716731434021296\n",
      "- learning... 214/10000 - loss:6.737141502650144\n",
      "- learning... 215/10000 - loss:6.713278684786286\n",
      "- learning... 216/10000 - loss:6.738701376607683\n",
      "- learning... 217/10000 - loss:6.749480751479171\n",
      "- learning... 218/10000 - loss:6.746369811509039\n",
      "- learning... 219/10000 - loss:6.752362541071898\n",
      "- learning... 220/10000 - loss:6.758429155865783\n",
      "- learning... 221/10000 - loss:6.715749999026383\n",
      "- learning... 222/10000 - loss:6.717035013933678\n",
      "- learning... 223/10000 - loss:6.714140437681177\n",
      "- learning... 224/10000 - loss:6.689707745170475\n",
      "- learning... 225/10000 - loss:6.709410820372629\n",
      "- learning... 226/10000 - loss:6.716865905524141\n",
      "- learning... 227/10000 - loss:6.708877141471535\n",
      "- learning... 228/10000 - loss:6.7187293903894\n",
      "- learning... 229/10000 - loss:6.7178388406002805\n",
      "- learning... 230/10000 - loss:6.640440279104642\n",
      "- learning... 231/10000 - loss:6.713163986608716\n",
      "- learning... 232/10000 - loss:6.695045877400346\n",
      "- learning... 233/10000 - loss:6.641003064196029\n",
      "- learning... 234/10000 - loss:6.694907922518623\n",
      "- learning... 235/10000 - loss:6.654720927861084\n",
      "- learning... 236/10000 - loss:6.662088800763629\n",
      "- learning... 237/10000 - loss:6.6452632754691505\n",
      "- learning... 238/10000 - loss:6.677771384399017\n",
      "- learning... 239/10000 - loss:6.643107621808396\n",
      "- learning... 240/10000 - loss:6.654864771421102\n",
      "- learning... 241/10000 - loss:6.603213764734636\n",
      "- learning... 242/10000 - loss:6.643516709486432\n",
      "- learning... 243/10000 - loss:6.620669948605306\n",
      "- learning... 244/10000 - loss:6.649662153239103\n",
      "- learning... 245/10000 - loss:6.575907741392959\n",
      "- learning... 246/10000 - loss:6.581806896354191\n",
      "- learning... 247/10000 - loss:6.678214365256343\n",
      "- learning... 248/10000 - loss:6.620259436132482\n",
      "- learning... 249/10000 - loss:6.598598749315288\n",
      "- learning... 250/10000 - loss:6.6060385452960615\n",
      "- learning... 251/10000 - loss:6.574349904474475\n",
      "- learning... 252/10000 - loss:6.593643037476821\n",
      "- learning... 253/10000 - loss:6.606309953108987\n",
      "- learning... 254/10000 - loss:6.595431224031667\n",
      "- learning... 255/10000 - loss:6.5878095200709765\n",
      "- learning... 256/10000 - loss:6.600587395626283\n",
      "- learning... 257/10000 - loss:6.589865053265765\n",
      "- learning... 258/10000 - loss:6.549233399380219\n",
      "- learning... 259/10000 - loss:6.561009509550105\n",
      "- learning... 260/10000 - loss:6.555144109946423\n",
      "- learning... 261/10000 - loss:6.555994718955901\n",
      "- learning... 262/10000 - loss:6.540622759656132\n",
      "- learning... 263/10000 - loss:6.536485214920558\n",
      "- learning... 264/10000 - loss:6.509170141638828\n",
      "- learning... 265/10000 - loss:6.581848631217458\n",
      "- learning... 266/10000 - loss:6.478955785144375\n",
      "- learning... 267/10000 - loss:6.504298440143768\n",
      "- learning... 268/10000 - loss:6.497008514193601\n",
      "- learning... 269/10000 - loss:6.511109337918263\n",
      "- learning... 270/10000 - loss:6.522507625779469\n",
      "- learning... 271/10000 - loss:6.509192381401291\n",
      "- learning... 272/10000 - loss:6.5287985686948256\n",
      "- learning... 273/10000 - loss:6.509196832207907\n",
      "- learning... 274/10000 - loss:6.4890670087444535\n",
      "- learning... 275/10000 - loss:6.529316804024944\n",
      "- learning... 276/10000 - loss:6.476567337423084\n",
      "- learning... 277/10000 - loss:6.4967473036480365\n",
      "- learning... 278/10000 - loss:6.451267458078002\n",
      "- learning... 279/10000 - loss:6.431274715599764\n",
      "- learning... 280/10000 - loss:6.405336061986862\n",
      "- learning... 281/10000 - loss:6.5180563560610025\n",
      "- learning... 282/10000 - loss:6.469154624343052\n",
      "- learning... 283/10000 - loss:6.445738753477441\n",
      "- learning... 284/10000 - loss:6.462537346167709\n",
      "- learning... 285/10000 - loss:6.375054514921531\n",
      "- learning... 286/10000 - loss:6.454353143516058\n",
      "- learning... 287/10000 - loss:6.452679769839271\n",
      "- learning... 288/10000 - loss:6.388467746462582\n",
      "- learning... 289/10000 - loss:6.42616158237615\n",
      "- learning... 290/10000 - loss:6.402126819610129\n",
      "- learning... 291/10000 - loss:6.391701727363284\n",
      "- learning... 292/10000 - loss:6.404396615654907\n",
      "- learning... 293/10000 - loss:6.399863841906855\n",
      "- learning... 294/10000 - loss:6.426956755104062\n",
      "- learning... 295/10000 - loss:6.364836274153846\n",
      "- learning... 296/10000 - loss:6.408027567774086\n",
      "- learning... 297/10000 - loss:6.347447725383996\n",
      "- learning... 298/10000 - loss:6.424879396303058\n",
      "- learning... 299/10000 - loss:6.404745576674368\n",
      "- learning... 300/10000 - loss:6.322142438016385\n",
      "- learning... 301/10000 - loss:6.427399472486851\n",
      "- learning... 302/10000 - loss:6.412903240427932\n",
      "- learning... 303/10000 - loss:6.337041790548601\n",
      "- learning... 304/10000 - loss:6.353417652642047\n",
      "- learning... 305/10000 - loss:6.311216256006381\n",
      "- learning... 306/10000 - loss:6.37996103147742\n",
      "- learning... 307/10000 - loss:6.281977236313327\n",
      "- learning... 308/10000 - loss:6.295036853621578\n",
      "- learning... 309/10000 - loss:6.248808694403133\n",
      "- learning... 310/10000 - loss:6.286001664586694\n",
      "- learning... 311/10000 - loss:6.324367066394022\n",
      "- learning... 312/10000 - loss:6.317513745900116\n",
      "- learning... 313/10000 - loss:6.273862187655192\n",
      "- learning... 314/10000 - loss:6.318919748402188\n",
      "- learning... 315/10000 - loss:6.31233156418408\n",
      "- learning... 316/10000 - loss:6.300221448199941\n",
      "- learning... 317/10000 - loss:6.341925662277436\n",
      "- learning... 318/10000 - loss:6.317728328326772\n",
      "- learning... 319/10000 - loss:6.3054806370726935\n",
      "- learning... 320/10000 - loss:6.298098577255747\n",
      "- learning... 321/10000 - loss:6.250245506875256\n",
      "- learning... 322/10000 - loss:6.313456998946126\n",
      "- learning... 323/10000 - loss:6.182610602218321\n",
      "- learning... 324/10000 - loss:6.200051486428416\n",
      "- learning... 325/10000 - loss:6.229065820040336\n",
      "- learning... 326/10000 - loss:6.231809513014814\n",
      "- learning... 327/10000 - loss:6.234919813899959\n",
      "- learning... 328/10000 - loss:6.304748779909371\n",
      "- learning... 329/10000 - loss:6.251773846962035\n",
      "- learning... 330/10000 - loss:6.177915159612519\n",
      "- learning... 331/10000 - loss:6.175792576179666\n",
      "- learning... 332/10000 - loss:6.2095280793117755\n",
      "- learning... 333/10000 - loss:6.252527371074933\n",
      "- learning... 334/10000 - loss:6.2305352304462485\n",
      "- learning... 335/10000 - loss:6.191663174429145\n",
      "- learning... 336/10000 - loss:6.281108824532298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- learning... 337/10000 - loss:6.1799514173942445\n",
      "- learning... 338/10000 - loss:6.207241528214099\n",
      "- learning... 339/10000 - loss:6.218217350625894\n",
      "- learning... 340/10000 - loss:6.236697846705556\n",
      "- learning... 341/10000 - loss:6.191155695375988\n",
      "- learning... 342/10000 - loss:6.152675341630359\n",
      "- learning... 343/10000 - loss:6.139389517721652\n",
      "- learning... 344/10000 - loss:6.172620572405332\n",
      "- learning... 345/10000 - loss:6.221334728800395\n",
      "- learning... 346/10000 - loss:6.148980107215028\n",
      "- learning... 347/10000 - loss:6.106844613446921\n",
      "- learning... 348/10000 - loss:6.210610880116433\n",
      "- learning... 349/10000 - loss:6.190269362641682\n",
      "- learning... 350/10000 - loss:6.1878121078701\n",
      "- learning... 351/10000 - loss:6.155146785521922\n",
      "- learning... 352/10000 - loss:6.20205338421145\n",
      "- learning... 353/10000 - loss:6.137153806464084\n",
      "- learning... 354/10000 - loss:6.190159299941878\n",
      "- learning... 355/10000 - loss:6.242178038827756\n",
      "- learning... 356/10000 - loss:6.17114959836259\n",
      "- learning... 357/10000 - loss:6.074972983431804\n",
      "- learning... 358/10000 - loss:6.105186795764772\n",
      "- learning... 359/10000 - loss:6.1628242153902555\n",
      "- learning... 360/10000 - loss:6.125977378029409\n",
      "- learning... 361/10000 - loss:6.109279913074067\n",
      "- learning... 362/10000 - loss:6.138357394140525\n",
      "- learning... 363/10000 - loss:6.011234365604335\n",
      "- learning... 364/10000 - loss:6.034069922212764\n",
      "- learning... 365/10000 - loss:6.016156953058077\n",
      "- learning... 366/10000 - loss:6.025406195999624\n",
      "- learning... 367/10000 - loss:6.1336048459959285\n",
      "- learning... 368/10000 - loss:6.033133109032982\n",
      "- learning... 369/10000 - loss:6.034405120427509\n",
      "- learning... 370/10000 - loss:6.092688537806969\n",
      "- learning... 371/10000 - loss:6.093641496611079\n",
      "- learning... 372/10000 - loss:6.020927116774494\n",
      "- learning... 373/10000 - loss:6.040762651008055\n",
      "- learning... 374/10000 - loss:6.123717880080811\n",
      "- learning... 375/10000 - loss:6.123593399628641\n",
      "- learning... 376/10000 - loss:6.095344434173478\n",
      "- learning... 377/10000 - loss:6.121470569482634\n",
      "- learning... 378/10000 - loss:6.0412333585708335\n",
      "- learning... 379/10000 - loss:5.975918700884883\n",
      "- learning... 380/10000 - loss:6.026014398007458\n",
      "- learning... 381/10000 - loss:6.007239114282933\n",
      "- learning... 382/10000 - loss:6.0963070631681715\n",
      "- learning... 383/10000 - loss:6.059026306533057\n",
      "- learning... 384/10000 - loss:5.940574272083085\n",
      "- learning... 385/10000 - loss:6.075457010232764\n",
      "- learning... 386/10000 - loss:6.023137735017903\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"./deep-learning-from-scratch-master/\")\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "class TwoLayerNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "    \n",
    "    def sigmoid_function(self, x):\n",
    "        return 1 / (1 + np.exp(-x)) # exp(x)はネイピア数（e）のx乗\n",
    "\n",
    "    def softmax_function(self, a):\n",
    "        c = np.max(a)\n",
    "        e_a = np.exp(a - c) # オーバーフロー対策\n",
    "        sum_e_a = np.sum(e_a)\n",
    "        return e_a / sum_e_a\n",
    "\n",
    "    def cross_entropy_error(self, y, t):\n",
    "        if y.ndim == 1:\n",
    "            t = t.reshape(1, t.size)\n",
    "            y = y.reshape(1, y.size)\n",
    "        # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "        if t.size == y.size:\n",
    "            t = t.argmax(axis=1) \n",
    "        batch_size = y.shape[0]\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params[\"W1\"], self.params[\"W2\"]\n",
    "        b1, b2 = self.params[\"b1\"], self.params[\"b2\"]\n",
    "\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = self.sigmoid_function(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = self.softmax_function(a2)\n",
    "        return y\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.cross_entropy_error(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, f, x):\n",
    "        h = 1e-4 # 0.0001\n",
    "        grad = np.zeros_like(x)\n",
    "        it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            tmp_val = x[idx]\n",
    "            x[idx] = float(tmp_val) + h # self.params[\"x\"]の更新（numpyのため参照渡しになる）\n",
    "            fxh1 = f(x) # f(x+h) -> xはダミーの引数。loss引数xには、gradient引数xが渡される\n",
    "        \n",
    "            x[idx] = tmp_val - h \n",
    "            fxh2 = f(x) # f(x-h)\n",
    "            grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "            x[idx] = tmp_val # 値を元に戻す\n",
    "            it.iternext()   \n",
    "        return grad\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads[\"W1\"] = self.numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads[\"b1\"] = self.numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads[\"W2\"] = self.numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads[\"b2\"] = self.numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "        return grads\n",
    "\n",
    "# NN = TwoLayerNN(input_size=784, hidden_size=100, output_size=10)\n",
    "# x = np.random.rand(100, 784)\n",
    "# t = np.random.rand(100, 10)\n",
    "# y = NN.predict(x)\n",
    "# grads = NN.gradient(x, t)\n",
    "# print(grads[\"W1\"].shape)\n",
    "\n",
    "# --- ミニバッチ学習 ---\n",
    "print(\"--- mini batch learning ---\")\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# ハイパーパラメータ\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 1エポックあたりの繰り返し数\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "NN = TwoLayerNN(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # ミニバッチの取得\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    # print(\"batch_mask : \" + str(batch_mask))\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    # 勾配の計算\n",
    "    grads = NN.gradient(x_batch, t_batch)\n",
    "\n",
    "    # パラメータの更新\n",
    "    for key in (\"W1\", \"W2\", \"b1\", \"b2\"):\n",
    "        NN.params[key] = NN.params[key] - (learning_rate * grads[key])\n",
    "    \n",
    "    # 学習経過の記録\n",
    "    loss = NN.loss(x_batch, t_batch)\n",
    "    print(\"- learning... \" + str(i) + \"/\" + str(iters_num) + \" - loss:\" + str(loss))\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1エポックごとに認識精度を計算\n",
    "    if 1 % iter_per_epoch == 0:\n",
    "        train_acc_list.append(NN.accuracy(x_train, t_train))\n",
    "        test_acc_list.append(NN.accuracy(x_train, t_train))\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
